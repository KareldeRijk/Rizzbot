{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is pre-summary generation notebook v1.1, as the previous version worked but generated 129k summaries. Additionally, this version generates summaries of 1000 words instaed of \n",
    "# 500 words, so we can inspect the difference in quality and nuance of ideas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66958c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\karel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 18:13:12,276 - ERROR - Configuration file config.yaml not found\n",
      "2025-07-01 18:13:12,277 - ERROR - Script failed: [Errno 2] No such file or directory: 'config.yaml'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 457\u001b[39m\n\u001b[32m    448\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    451\u001b[39m     \u001b[38;5;66;03m# You can choose to run both or just one:\u001b[39;00m\n\u001b[32m    452\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    455\u001b[39m \n\u001b[32m    456\u001b[39m     \u001b[38;5;66;03m# Option 2: Run just 1000-word summaries\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     \u001b[43mrun_single_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_count\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m# Option 3: Run just 500-word summaries  \u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m# run_single_batch(word_count=500)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 439\u001b[39m, in \u001b[36mrun_single_batch\u001b[39m\u001b[34m(word_count)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    438\u001b[39m     run_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mword_summaries_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     summarizer = \u001b[43mTopicSummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mword_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m     results = summarizer.run()\n\u001b[32m    442\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== RESULTS ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mTopicSummarizer.__init__\u001b[39m\u001b[34m(self, config_file, summary_words, run_name)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config_file=\u001b[33m\"\u001b[39m\u001b[33mconfig.yaml\u001b[39m\u001b[33m\"\u001b[39m, summary_words=\u001b[32m500\u001b[39m, run_name=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28mself\u001b[39m.config = \u001b[43mConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mself\u001b[39m.run_name = run_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mword_run_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28mself\u001b[39m.s3_client = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mConfig.__init__\u001b[39m\u001b[34m(self, config_file, summary_words)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config_file=\u001b[33m\"\u001b[39m\u001b[33mconfig.yaml\u001b[39m\u001b[33m\"\u001b[39m, summary_words=\u001b[32m500\u001b[39m):\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Load configuration from YAML file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28mself\u001b[39m.config_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# File paths\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mself\u001b[39m.CLUSTERED_VECTORS_PATH = \u001b[33m\"\u001b[39m\u001b[33mrizzbot_data/cleaned_clustered_vectors.pkl\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mConfig.load_config\u001b[39m\u001b[34m(self, config_file)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load configuration from YAML file\"\"\"\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     66\u001b[39m         config = yaml.safe_load(f)\n\u001b[32m     67\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded configuration from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'config.yaml'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Union\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------- Configure Logging ----------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('topic_summarization.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------- Config ----------\n",
    "class Config:\n",
    "    def __init__(self, config_file=\"config.yaml\", summary_words=500):\n",
    "        # Load configuration from YAML file\n",
    "        self.config_data = self.load_config(config_file)\n",
    "        \n",
    "        # File paths\n",
    "        self.CLUSTERED_VECTORS_PATH = \"rizzbot_data/cleaned_clustered_vectors.pkl\"\n",
    "        self.TOPIC_MODEL_PATH = \"rizzbot_data/bertopic_model\"\n",
    "        \n",
    "        # S3 settings\n",
    "        self.S3_BUCKET = \"rizzbot-temp-storage\"\n",
    "        self.S3_PREFIX = \"rizzbot/Summaries/\"\n",
    "        \n",
    "        # Pinecone settings\n",
    "        self.PINECONE_INDEX = \"rizzbot-summaries\"\n",
    "        \n",
    "        # Model settings\n",
    "        self.EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "        self.SUMMARY_MODEL = \"gpt-4o-mini\"\n",
    "        \n",
    "        # Processing settings - NOW CONFIGURABLE\n",
    "        self.MAX_SUMMARY_WORDS = summary_words  # Can be set to 500 or 1000\n",
    "        self.MAX_DOCS_PER_TOPIC = 50\n",
    "        self.CHUNK_SIZE = 500\n",
    "        \n",
    "        # API Keys from config.yaml\n",
    "        self.OPENAI_API_KEY = self.config_data.get('openai_api_key')\n",
    "        self.PINECONE_API_KEY = self.config_data.get('pinecone_api_key')\n",
    "        \n",
    "        # Validate required keys\n",
    "        self.validate_config()\n",
    "    \n",
    "    def load_config(self, config_file: str) -> dict:\n",
    "        \"\"\"Load configuration from YAML file\"\"\"\n",
    "        try:\n",
    "            with open(config_file, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            logger.info(f\"Loaded configuration from {config_file}\")\n",
    "            return config\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Configuration file {config_file} not found\")\n",
    "            raise\n",
    "        except yaml.YAMLError as e:\n",
    "            logger.error(f\"Error parsing YAML file: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def validate_config(self):\n",
    "        \"\"\"Validate that required API keys are present\"\"\"\n",
    "        missing_keys = []\n",
    "        \n",
    "        if not self.OPENAI_API_KEY:\n",
    "            missing_keys.append('openai_api_key')\n",
    "        \n",
    "        if not self.PINECONE_API_KEY:\n",
    "            missing_keys.append('pinecone_api_key')\n",
    "        \n",
    "        if missing_keys:\n",
    "            logger.error(f\"Missing required API keys in config.yaml: {missing_keys}\")\n",
    "            raise ValueError(f\"Missing required API keys: {', '.join(missing_keys)}\")\n",
    "        \n",
    "        logger.info(\"All required API keys found in configuration\")\n",
    "\n",
    "class TopicSummarizer:\n",
    "    def __init__(self, config_file=\"config.yaml\", summary_words=500, run_name=None):\n",
    "        self.config = Config(config_file, summary_words)\n",
    "        self.run_name = run_name or f\"{summary_words}word_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.s3_client = None\n",
    "        self.openai_client = None\n",
    "        self.pinecone_client = None\n",
    "        self.pinecone_index = None\n",
    "        self.embedder = None\n",
    "        self.topic_model = None\n",
    "        self.clustered_vectors = None\n",
    "        \n",
    "        logger.info(f\"Initialized TopicSummarizer for {summary_words}-word summaries, run: {self.run_name}\")\n",
    "        \n",
    "    def initialize_clients(self):\n",
    "        \"\"\"Initialize all external service clients\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Initializing clients...\")\n",
    "            \n",
    "            # Initialize S3 client\n",
    "            self.s3_client = boto3.client(\"s3\")\n",
    "            logger.info(\"S3 client initialized\")\n",
    "            \n",
    "            # Initialize OpenAI client\n",
    "            self.openai_client = OpenAI(api_key=self.config.OPENAI_API_KEY)\n",
    "            logger.info(\"OpenAI client initialized\")\n",
    "            \n",
    "            # Test OpenAI connection\n",
    "            try:\n",
    "                test_response = self.openai_client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                    max_tokens=5\n",
    "                )\n",
    "                logger.info(\"OpenAI API connection test successful\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"OpenAI API connection test failed: {e}\")\n",
    "                raise\n",
    "            \n",
    "            # Initialize Pinecone client\n",
    "            self.pinecone_client = Pinecone(api_key=self.config.PINECONE_API_KEY)\n",
    "            self.pinecone_index = self.pinecone_client.Index(self.config.PINECONE_INDEX)\n",
    "            logger.info(\"Pinecone client initialized\")\n",
    "            \n",
    "            # Initialize sentence transformer\n",
    "            self.embedder = SentenceTransformer(self.config.EMBEDDING_MODEL)\n",
    "            logger.info(\"Sentence transformer initialized\")\n",
    "            \n",
    "            logger.info(\"All clients initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize clients: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load clustered data and BERTopic model\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Loading clustered vectors...\")\n",
    "            with open(self.config.CLUSTERED_VECTORS_PATH, \"rb\") as f:\n",
    "                self.clustered_vectors = pickle.load(f)\n",
    "            logger.info(f\"Loaded clustered vectors: {len(self.clustered_vectors)} items\")\n",
    "            \n",
    "            if isinstance(self.clustered_vectors, pd.DataFrame):\n",
    "                logger.info(f\"DataFrame shape: {self.clustered_vectors.shape}, columns: {list(self.clustered_vectors.columns)}\")\n",
    "            else:\n",
    "                logger.info(f\"Data type: {type(self.clustered_vectors)}\")\n",
    "\n",
    "            self.load_topic_model()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def load_topic_model(self):\n",
    "        \"\"\"Load BERTopic model with re-attached embedding model\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading BERTopic model from: {self.config.TOPIC_MODEL_PATH}\")\n",
    "            embedding_model = SentenceTransformer(self.config.EMBEDDING_MODEL)\n",
    "            self.topic_model = BERTopic.load(self.config.TOPIC_MODEL_PATH, embedding_model=embedding_model)\n",
    "            topics = self.topic_model.get_topics()\n",
    "            logger.info(f\"Loaded BERTopic model with {len(topics)} topics\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading BERTopic model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def group_documents_by_topic(self) -> Dict[int, List[str]]:\n",
    "        \"\"\"Group documents by their topic ID\"\"\"\n",
    "        logger.info(\"Grouping documents by topic...\")\n",
    "        topic_to_docs = {}\n",
    "\n",
    "        if isinstance(self.clustered_vectors, pd.DataFrame):\n",
    "            df = self.clustered_vectors\n",
    "            if 'topic_id' not in df.columns or 'text' not in df.columns:\n",
    "                raise ValueError(\"clustered_vectors must include 'topic_id' and 'text'\")\n",
    "            for topic_id, group in df.dropna(subset=['topic_id', 'text']).groupby('topic_id'):\n",
    "                if topic_id == -1:  # Skip noise cluster\n",
    "                    continue\n",
    "                topic_to_docs[int(topic_id)] = group['text'].tolist()\n",
    "        else:\n",
    "            for item in self.clustered_vectors:\n",
    "                if not isinstance(item, dict): \n",
    "                    continue\n",
    "                topic_id = item.get(\"topic_id\")\n",
    "                text = item.get(\"text\")\n",
    "                if topic_id is not None and text and topic_id != -1:\n",
    "                    topic_to_docs.setdefault(topic_id, []).append(text)\n",
    "\n",
    "        logger.info(f\"Grouped documents into {len(topic_to_docs)} topics\")\n",
    "        return topic_to_docs\n",
    "\n",
    "    def generate_summary(self, topic_id: int, docs: List[str]) -> str:\n",
    "        \"\"\"Generate summary for a single topic\"\"\"\n",
    "        try:\n",
    "            combined_text = \"\\n\\n\".join(docs[:self.config.MAX_DOCS_PER_TOPIC])\n",
    "            \n",
    "            logger.info(f\"Generating {self.config.MAX_SUMMARY_WORDS}-word summary for topic {topic_id} with {len(docs)} documents\")\n",
    "            logger.info(f\"Combined text length: {len(combined_text)} characters\")\n",
    "            \n",
    "            prompt = (\n",
    "                f\"You are a helpful assistant. Write a comprehensive and detailed {self.config.MAX_SUMMARY_WORDS}-word summary \"\n",
    "                f\"of the key themes, insights, and patterns found in the following documents. \"\n",
    "                f\"Focus on capturing the main ideas, important details, and any nuanced perspectives present in the text.\\n\\n\"\n",
    "                f\"Documents:\\n{combined_text}\\n\\n\"\n",
    "                f\"Please provide a {self.config.MAX_SUMMARY_WORDS}-word summary:\"\n",
    "            )\n",
    "            \n",
    "            # Adjust max_tokens based on summary length\n",
    "            max_tokens = int(self.config.MAX_SUMMARY_WORDS * 1.5)  # Allow some buffer\n",
    "            \n",
    "            response = self.openai_client.chat.completions.create(\n",
    "                model=self.config.SUMMARY_MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.5,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            summary = response.choices[0].message.content.strip()\n",
    "            logger.info(f\"Successfully generated summary for topic {topic_id}: {len(summary)} characters\")\n",
    "            return summary\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to generate summary for topic {topic_id}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def save_to_s3(self, topic_id: int, summary_text: str):\n",
    "        \"\"\"Save summary to S3\"\"\"\n",
    "        try:\n",
    "            s3_key = f\"{self.config.S3_PREFIX}{self.run_name}/topic_{topic_id}.json\"\n",
    "            \n",
    "            summary_data = {\n",
    "                \"topic_id\": topic_id,\n",
    "                \"summary_text\": summary_text,\n",
    "                \"source\": \"BERTopic\",\n",
    "                \"run_name\": self.run_name,\n",
    "                \"summary_word_target\": self.config.MAX_SUMMARY_WORDS,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"model_used\": self.config.SUMMARY_MODEL\n",
    "            }\n",
    "            \n",
    "            self.s3_client.put_object(\n",
    "                Bucket=self.config.S3_BUCKET,\n",
    "                Key=s3_key,\n",
    "                Body=json.dumps(summary_data, indent=2),\n",
    "                ContentType=\"application/json\"\n",
    "            )\n",
    "            logger.info(f\"Saved topic {topic_id} to S3: {s3_key}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save topic {topic_id} to S3: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def save_to_pinecone(self, topic_id: int, summary_text: str):\n",
    "        \"\"\"Save summary chunks to Pinecone\"\"\"\n",
    "        try:\n",
    "            chunks = [summary_text[i:i+self.config.CHUNK_SIZE] \n",
    "                     for i in range(0, len(summary_text), self.config.CHUNK_SIZE)]\n",
    "            \n",
    "            vectors_to_upsert = []\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                embedding = self.embedder.encode(chunk).tolist()\n",
    "                vector_id = f\"summary-{self.run_name}-{topic_id}-{i}\"\n",
    "                \n",
    "                metadata = {\n",
    "                    \"type\": \"summary\",\n",
    "                    \"topic_id\": str(topic_id),\n",
    "                    \"chunk_id\": i,\n",
    "                    \"source\": \"BERTopic\",\n",
    "                    \"summary_quality\": \"v1.0\",\n",
    "                    \"run_name\": self.run_name,\n",
    "                    \"summary_word_target\": self.config.MAX_SUMMARY_WORDS,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"model_used\": self.config.SUMMARY_MODEL\n",
    "                }\n",
    "                \n",
    "                vectors_to_upsert.append((vector_id, embedding, metadata))\n",
    "            \n",
    "            # Batch upsert for efficiency\n",
    "            self.pinecone_index.upsert(vectors=vectors_to_upsert)\n",
    "            logger.info(f\"Saved {len(chunks)} chunks for topic {topic_id} to Pinecone\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save topic {topic_id} to Pinecone: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def save_summaries_locally(self, summaries: Dict[int, str]):\n",
    "        \"\"\"Save summaries to local file for backup\"\"\"\n",
    "        try:\n",
    "            local_dir = f\"rizzbot_data/summaries_{self.run_name}\"\n",
    "            os.makedirs(local_dir, exist_ok=True)\n",
    "            \n",
    "            # Save individual summaries\n",
    "            for topic_id, summary_text in summaries.items():\n",
    "                filename = os.path.join(local_dir, f\"topic_{topic_id}.txt\")\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(f\"Topic ID: {topic_id}\\n\")\n",
    "                    f.write(f\"Run Name: {self.run_name}\\n\")\n",
    "                    f.write(f\"Target Words: {self.config.MAX_SUMMARY_WORDS}\\n\")\n",
    "                    f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
    "                    f.write(f\"Model: {self.config.SUMMARY_MODEL}\\n\")\n",
    "                    f.write(\"-\" * 50 + \"\\n\")\n",
    "                    f.write(summary_text)\n",
    "            \n",
    "            # Save consolidated file\n",
    "            consolidated_file = os.path.join(local_dir, \"all_summaries.json\")\n",
    "            with open(consolidated_file, 'w', encoding='utf-8') as f:\n",
    "                summary_data = {\n",
    "                    \"run_name\": self.run_name,\n",
    "                    \"summary_word_target\": self.config.MAX_SUMMARY_WORDS,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"model_used\": self.config.SUMMARY_MODEL,\n",
    "                    \"total_topics\": len(summaries),\n",
    "                    \"summaries\": summaries\n",
    "                }\n",
    "                json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(f\"Saved {len(summaries)} summaries locally in {local_dir}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save summaries locally: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def run_summarization(self) -> Dict[int, str]:\n",
    "        \"\"\"Run summarization for all topics - FIXED VERSION\"\"\"\n",
    "        topic_to_docs = self.group_documents_by_topic()\n",
    "        summaries = {}\n",
    "        failed_topics = []\n",
    "\n",
    "        logger.info(f\"Starting {self.config.MAX_SUMMARY_WORDS}-word summarization for {len(topic_to_docs)} topics...\")\n",
    "\n",
    "        for topic_id in tqdm(topic_to_docs, desc=f\"Generating {self.config.MAX_SUMMARY_WORDS}-word summaries\"):\n",
    "            try:\n",
    "                docs = topic_to_docs[topic_id]\n",
    "                summary = self.generate_summary(topic_id, docs)\n",
    "                summaries[topic_id] = summary\n",
    "                \n",
    "                # Save to external services\n",
    "                self.save_to_s3(topic_id, summary)\n",
    "                self.save_to_pinecone(topic_id, summary)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Topic {topic_id} failed: {e}\")\n",
    "                failed_topics.append(topic_id)\n",
    "\n",
    "        # Save local backup\n",
    "        self.save_summaries_locally(summaries)\n",
    "        \n",
    "        logger.info(f\"Summarization completed for {self.run_name}\")\n",
    "        logger.info(f\"Successful: {len(summaries)} | Failed: {len(failed_topics)}\")\n",
    "        \n",
    "        if failed_topics:\n",
    "            logger.warning(f\"Failed topics: {failed_topics}\")\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    def run(self) -> Dict[int, str]:\n",
    "        \"\"\"Main execution method - SIMPLIFIED\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting TopicSummarizer run: {self.run_name}\")\n",
    "            self.initialize_clients()\n",
    "            self.load_data()\n",
    "            summaries = self.run_summarization()\n",
    "            logger.info(f\"Run {self.run_name} completed successfully with {len(summaries)} summaries\")\n",
    "            return summaries\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fatal error during execution: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point with support for different word counts\"\"\"\n",
    "    try:\n",
    "        # Run 500-word summaries\n",
    "        print(\"=== Running 500-word summaries ===\")\n",
    "        summarizer_500 = TopicSummarizer(\"config.yaml\", summary_words=500, run_name=\"500word_summaries\")\n",
    "        results_500 = summarizer_500.run()\n",
    "        print(f\"500-word summaries completed: {len(results_500)} summaries generated\")\n",
    "        \n",
    "        # Run 1000-word summaries  \n",
    "        print(\"\\n=== Running 1000-word summaries ===\")\n",
    "        summarizer_1000 = TopicSummarizer(\"config.yaml\", summary_words=1000, run_name=\"1000word_summaries\")\n",
    "        results_1000 = summarizer_1000.run()\n",
    "        print(f\"1000-word summaries completed: {len(results_1000)} summaries generated\")\n",
    "        \n",
    "        # Final summary\n",
    "        print(f\"\\n=== FINAL RESULTS ===\")\n",
    "        print(f\"500-word summaries: {len(results_500)}\")\n",
    "        print(f\"1000-word summaries: {len(results_1000)}\")\n",
    "        print(f\"Total summaries generated: {len(results_500) + len(results_1000)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def run_single_batch(word_count=500):\n",
    "    \"\"\"Helper function to run just one batch of summaries\"\"\"\n",
    "    try:\n",
    "        run_name = f\"{word_count}word_summaries_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        summarizer = TopicSummarizer(\"config.yaml\", summary_words=word_count, run_name=run_name)\n",
    "        results = summarizer.run()\n",
    "        \n",
    "        print(f\"\\n=== RESULTS ===\")\n",
    "        print(f\"Generated {len(results)} summaries of {word_count} words each\")\n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script failed: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # You can choose to run both or just one:\n",
    "    \n",
    "    # Option 1: Run both 500 and 1000 word summaries\n",
    "    # main()\n",
    "    \n",
    "    # Option 2: Run just 1000-word summaries\n",
    "    run_single_batch(word_count=1000)\n",
    "    \n",
    "    # Option 3: Run just 500-word summaries  \n",
    "    # run_single_batch(word_count=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d9fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client in c:\\users\\karel\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.1.0)\n",
      "Collecting pinecone-client\n",
      "  Using cached pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\karel\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (2025.1.31)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\karel\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\karel\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (2.9.0.post0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\karel\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (4.13.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\karel\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\karel\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
      "Using cached pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
      "Installing collected packages: pinecone-client\n",
      "  Attempting uninstall: pinecone-client\n",
      "    Found existing installation: pinecone-client 3.1.0\n",
      "    Uninstalling pinecone-client-3.1.0:\n",
      "      Successfully uninstalled pinecone-client-3.1.0\n",
      "Successfully installed pinecone-client-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pinecone-client"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
